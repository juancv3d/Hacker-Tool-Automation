
import requests
import urllib.parse
import re


class Scanner():
    def __init__(self, url):
        self.url = url
        self.target_links = []

    def extract_links_from(self, url):
        """
        extract all the links from the given url.

        """
        try:
            response = requests.get(url)
            return re.findall('(?:href=")(.*?)"', str(response.content))
        except requests.exceptions.ConnectionError:
            print("Connection refused")

    def crawler(self, url):
        """
        This function is the main crawler function. and it will be called recursively to search for all the links in the given url.

        Args:
            url (string): The url to be crawled.
        """
        href_links = self.extract_links_from(url)
        for link in href_links:
            link = urllib.parse.urljoin(url, link)

            if '#' in link:
                link = link.split('#')[0]

            if url in link and link not in self.target_links:
                self.target_links.append(link)
                print(link)
                self.crawler(link)
